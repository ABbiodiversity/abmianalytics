---
title: "ARU file screening"
author: "Peter Solymos"
date: "Aug 24, 2016"
output:
  word_document: default
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: no
---

## Intro

Audio files from each ARU unit are dumped into neatly organized directories
on hard drives and servers after retrieval.
Checking these files requires someone to: bulk-convert the WAC files to WAV
and look at them individually to check various features of the files.
The goal is to eliminate files with defects that render them unsuitable for
listening. Such defects most often include: recording stopped short 
(<3 or 10 minutes duration), left, right, or both channel is useless (mic problem).
This approach is time consuming because of the file conversion and
human involvement (due to clicking in a graphical user interface, auditory/visual
checking, etc.)

Ways to speed up this part of the QA/QC process is to skip the conversion altogether,
or to automate as much of the process as possible.
My goal here is to find an efficient way to screen the files and reduce
human intervention in the process. I am discussing some ideas about
automating the inspection of converted WAC files, but I focus more
on things that can be done based on original WAC file attributes.

## Features of WAC and WAV files

WAC is a compressed (35-70% compression
compared to WAV) file format created by Wildlife Acoustics.
The only useful attribute of the compressed file is its size. Mor on it later.

The WAC source code was opened in 2014 under GPL license (see [here](http://peter.solymos.org/code/2016/03/14/wac2wav-converter.html)).
A program written in C can be used to convert WAC to WAV, file conversion
based on a random sample of files takes on 4.6-7.3 seconds for 3 minutes (mean = 5.4 sec, $n=191$)
and 14.9-22.7 seconds for 10 minutes files (mean = 16.2 sec, $n=59$).
This does not seem a lot for individual files, but the 2015 ABMI ARU sampling
resulted in about 339 thousand WAC files.
Altogether, conversion of 339K files would take 30 days on a single computer.


```{r eval=TRUE,echo=FALSE,message=FALSE}
load("e:\\peter\\AB_data_v2016\\out\\aru-filtering\\filter-dirs.Rdata")
load("e:\\peter\\AB_data_v2016\\out\\aru-filtering\\dirs-files.Rdata")
load("e:\\peter\\AB_data_v2016\\out\\aru-filtering\\sample.Rdata")

p_time <- sapply(RES, "[[", "timer")
dur <- sapply(RES, "[[", "duration")
summary(p_time[dur == 180])
summary(p_time[dur == 600])
```

After conversion, the WAV file can be further manipulated using the **tuneR**,
and **seewave** R packages.
Stereo WAV files store audio bitstreams, in our case
aplitudes (dB) sampled at 48000/sec rate with 16 bits per sample for left and right channels.
The following 2 plots show a 3 minutes (180 sec) fole with both channels fine.
The second one is a 10 minutes (600 sec) recording from a unit where
the right microphone was already dead on deployment.


```{r echo=FALSE,message=FALSE}
library(tuneR)
library(seewave)
DIRIN <- "u:\\2015_AllData\\ABMI-Core\\ABMI-0032\\ABMI-0032-NW"
DIROUT <- "e:\\Peter\\tmp"
CPROG <- "C:\\Users\\Peter\\Dropbox\\abmi\\aru\\wac2wav\\wac2wav.exe"
aru_convert <-
function(DIRIN, DIROUT, FNAME, CPROG)
{
    ## fin & fout must be full path
    fin <- file.path(DIRIN, FNAME, fsep="\\")
    fout <- file.path(DIROUT, gsub(".wac", ".wav", FNAME), fsep="\\")
    CALL <- paste(CPROG, fin, fout)
    system(CALL)
    invisible(fout)
}
aru_parse_location <-
function(f)
{
    f <- gsub(".wac", "", f)
    f <- sapply(strsplit(f, "\\+"), "[[", 1L)
    substr(f, 1, nchar(f)-2)
}
aru_parse_date <-
function(f, type=c("full", "date", "time"))
{
    type <- match.arg(type)
    f <- gsub(".wac", "", f)
    if (type == "full") {
        f <- substr(sapply(strsplit(f, "\\+"), "[[", 2L), 3, nchar(f))
        out <- as.POSIXlt(strptime(f, "%Y%m%d_%H%M%S"))
    } else {
        out <- sapply(strsplit(f, "_"), "[[", if (type == "date") 3L else 4L)
    }
    out
}
aru_calculateH <-
function (wave, f, wl = 512, envt = "hil", msmooth = NULL, ksmooth = NULL)
{
    input <- inputw(wave = wave, f = f)
    wave <- input$w
    f <- input$f
    rm(input)
    spec <- meanspec(wave = wave, f = f, wl = wl, plot = FALSE)
    SH <- sh(spec)
    enve <- env(wave = wave, f = f, envt = envt, msmooth = msmooth,
        ksmooth = ksmooth, plot = FALSE)
    TH <- th(enve)
    H <- SH * TH
    return(c(H=H, SH=SH, TH=TH))
}
aru_leftright <- function(object) {
    d <- object@left - object@right
    sdl <- sd(object@left)
    sdr <- sd(object@right)
    c(sd_left=sdl, sd_right=sdr,
        mean_left=mean(object@left), mean_right=mean(object@right),
        min_left=min(object@left), min_right=min(object@right),
        max_left=max(object@left), max_right=max(object@right),
        std_diff = mean(d / (0.5*(sdl+sdr))),
        log_sd_ratio = log(sd(object@left) / sd(object@right)))
}
aru_summarize <-
function(DIRIN, DIROUT, FNAME, CPROG, keep_wav=FALSE, quick=FALSE, return_object=TRUE)
{
    t0 <- proc.time()
    if (!file.exists(file.path(DIRIN, FNAME, fsep="\\")))
        stop("file does not exist")
    FNWAV <- try(aru_convert(DIRIN, DIROUT, FNAME, CPROG))
    if (inherits(FNWAV, "try-error"))
        stop("wac2wav error")
    object <- readWave(FNWAV)
    #object <- if (sample_p < 1)
    #    downsample(object0, samp.rate=sample_p*object@samp.rate) else object0
    #SPECTRO <- spectro(object, plot=FALSE)
    #CSH <- csh(object, plot=FALSE) # range, mean etc ?
    H <- if (quick)
        NULL else aru_calculateH(object)
    out <- list(file=FNAME,
        timer = unname(proc.time() - t0)[3L],
        location = aru_parse_location(FNAME),
        date = aru_parse_date(FNAME),
        duration = length(object@left)/object@samp.rate,
        size_wac = file.size(file.path(DIRIN, FNAME, fsep="\\")),
        size_wav = file.size(FNWAV),
        entropy=H,
        channels=aru_leftright(object),
        #sample_p=sample_p,
        object=if (return_object) object else NULL)
    if (!keep_wav)
        file.remove(FNWAV)
    out
}
## short file
res0 <- aru_summarize(
    FNAME = "ABMI-0032-NW_0+1_20150723_020255.wac",
    DIRIN = "u:\\2015_AllData\\ABMI-Core\\ABMI-0032\\ABMI-0032-NW",
    DIROUT = "e:\\Peter\\tmp",
    CPROG = "C:\\Users\\Peter\\Dropbox\\abmi\\aru\\wac2wav\\wac2wav.exe",
    keep_wav = TRUE, quick=TRUE)
## Right microphone already dead on deployment recording
res1 <- aru_summarize(
    FNAME = "ABMI-0331-NW_0+1_20150301_090000.wac",
    DIRIN = "u:\\2015_AllData\\ABMI-Core\\ABMI-0331\\ABMI-0331-NW",
    DIROUT = "e:\\Peter\\tmp",
    CPROG = "C:\\Users\\Peter\\Dropbox\\abmi\\aru\\wac2wav\\wac2wav.exe",
    keep_wav = TRUE, quick=TRUE)
## OK file
res2 <- aru_summarize(
    FNAME = "ABMI-0388-SW_0+1_20150626_071600.wac",
    DIRIN = "u:\\2015_AllData\\ABMI-Core\\ABMI-0388\\ABMI-0388-SW",
    DIROUT = "e:\\Peter\\tmp",
    CPROG = "C:\\Users\\Peter\\Dropbox\\abmi\\aru\\wac2wav\\wac2wav.exe",
    keep_wav = TRUE, quick=TRUE)
```

```{r echo=FALSE,fig.height=4,fig.width=7}
plot(res2$object, main="ABMI-0388-SW_0+1_20150626_071600.wac", col=4)
plot(res1$object, main="ABMI-0331-NW_0+1_20150301_090000.wac", col=4)
```

The left/right imbalance can be detected automatically by
comparing the the SD's of the channels. In the 1st case 
of the good file we get
$SD_{left}$ = `r unname(round(res2$channels["sd_left"],2))`
and $SD_{right}$ = `r unname(round(res2$channels["sd_right"],2))`.
In the 2nd case of the broken right channel we get
$SD_{left}$ = `r unname(round(res1$channels["sd_left"],2))`
and $SD_{right}$ = `r unname(round(res1$channels["sd_right"],2))`.
The ratio of the SD's can indicate channel issues (e.g.
$log(SD_{left}/SD_{left})$, values close to 0 indicating comparable channels).

Problems with microphones that a not intermittent can be detected by
inspecting few of the recordings (after deployment, mid-season, before retrieval).
I haven't purusued this line of research further, but will continue
after gathering the relevant information from Hedwig (e.g. known issues
with units, files, etc.), so that I can better assess accuracy of the
automated process.

Next I compared the file sizes of the WAC and WAV files to the recording duration
for the $n=260$ random sample.
These results show that WAV file size relationship with duration follow
a perfect line as expected, but WAC file size and duration relationship showed more
variation due to mostly noise level affecting compression.
By looking at the WAC-WAV file size graph, we can see that the unwanted
<180 second duration files would be easy to screen based on WAV file size,
but WAC file size present a problem. The file size for the smalled 3-minutes
file is smaller than the file size for the larges unwanted file.
If we want to screen the WAC files based on their size to eliminate the <3 minutes
files, we need to find a file size threshold that will minimize
the chances of 3-minutes files being eliminated.

```{r echo=FALSE,fig.height=4,fig.width=7}
swac <- sapply(RES, "[[", "size_wac")/1024^2
swav <- sapply(RES, "[[", "size_wav")/1024^2
op <- par(mfrow=c(1,3))
plot(dur, swac, xlab="Duration (sec)", ylab="WAC file size (Mb)")
abline(lm(swac ~ dur), col=2)
plot(dur, swav, xlab="Duration (sec)", ylab="WAV file size (Mb)")
abline(lm(swav ~ dur), col=2)
plot(swac, swav, xlab="WAC file size (Mb)", ylab="WAV file size (Mb)")
abline(lm(swav ~ swac), col=2)
par(op)
```

## Screening WAC files

I screened $n=266694$ files from 260 ARU units. Here is the WAC file size 
distribution for these files. The histogram shows the distribution
for all units, the next figure below shows the unit-specific variation
in file size ranges.

```{r echo=FALSE,fig.height=4,fig.width=7}
aswac <- unlist(lapply(RESbyDIR, function(z) z$size_wac))/1024^2
aswacr <- t(sapply(RESbyDIR, function(z) range(z$size_wac)))/1024^2
#aswacr1 <- t(sapply(RESbyDIR, function(z) 
#    range(z$size_wac[!is.na(z$pval) & z$pval<0.05])))/1024^2
#aswacr2 <- t(sapply(RESbyDIR, function(z) 
#    range(z$size_wac[is.na(z$pval) | z$pval>=0.05])))/1024^2
#op <- par(mfrow=c(2,1))
hist(aswac, xlab="WAC size (Mb)", main="", col="grey")
plot(0, type="n", 
    ylim=c(0, max(aswac, na.rm=TRUE)), xlim=c(0, length(RESbyDIR)+1),
    ylab="WAC size (Mb)", xlab="ARU unit")
for (i in 1:length(RESbyDIR)) {
    lines(c(i,i), aswacr[order(rowMeans(aswacr)),][i,], col=4)
#    lines(c(i,i), aswacr2[order(aswacr2[,1]),][i,], col=4)
#    lines(c(i,i), aswacr1[order(aswacr2[,1]),][i,], col=3)
}
#par(op)
```

Zooming in on a single ARU unit (ABMI-0001-NE).
The date-time distribution of recordings is shown in the
leftmost plot. The middle one depicts the 1st week
after deployment, the last one is about 1 week before retrieval.
The pattern seen here is quite common across the units.
The red dots are the files that are flagged as the ones that
are likely be <3 minutes in duration.

```{r echo=FALSE,fig.height=4,fig.width=7}
i <- 1
t0 <- proc.time()
DIR <- dirs[i]
FLWAC <- list.files(DIR)
FLWAC <- FLWAC[grep(".wac", FLWAC)]

LOC <- aru_parse_location(FLWAC[1L])
dd <- data.frame(file=FLWAC,
        full_date=aru_parse_date(FLWAC, type="full"),
        date=aru_parse_date(FLWAC, type="date"),
        time=aru_parse_date(FLWAC, type="time"),
        size_wac=sapply(FLWAC, function(z) file.size(file.path(DIR, z, fsep="\\"))))
rownames(dd) <- NULL
dd$yday <- as.POSIXlt(dd$full_date)$yday
dd$date2 <- as.POSIXlt(strptime(dd$date, "%Y%m%d"))
dd$time2 <- as.POSIXlt(strptime(dd$time, "%H%M%S"))

si <- dd$size_wac / 1024^2
names(si) <- dd$file
si <- sort(si[si < 40])
sid <- cbind(Theoretical=qnorm(seq(0.0001,0.9999,len=length(si))),
    Empirical=si,
    pval=pnorm(-abs((si-mean(si))/sd(si)))) # 1-sided
rownames(sid) <- names(si)
#sid[sid[,3] < 0.05,]
dd$pval <- sid[match(dd$file, rownames(sid)),3]
dd$flag <- FALSE
dd$flag[!is.na(dd$pval) & dd$pval < 0.05] <- TRUE
#dd <- dd[order(dd$size_wac),]
#dd$flag[which.max(dd$size_wac[dd$flag])+1] <- TRUE
#dd <- dd[order(dd$full_date),]
(t1 <- proc.time() - t0)

tmp <- as.matrix(aggregate(dd$size_wac, list(dd$yday), range))
tmp <- tmp[match(as.character(dd$yday), as.character(tmp[,1])),]
dd$dmin <- tmp[,2]
dd$dmax <- tmp[,3]
tmp <- as.matrix(aggregate(dd$size_wac, list(dd$yday), mean))
tmp <- tmp[match(as.character(dd$yday), as.character(tmp[,1])),]
dd$dmean <- tmp[,2]

op <- par(mfrow=c(1,3))

plot(dd$date2, dd$time2, pch=19, col=1, main=LOC,
    xlab="Date", ylab="Time")
with(dd[dd$pval < 0.05,], points(date2, time2, pch=19, col=2))

with(dd[dd$yday <= min(dd$yday)+7,], plot(full_date, size_wac/1024^2, type="n",
    main="", ylab="WAC file size (Mb)", xlab="1 week after deployment",
    ylim=c(0, max(dd$size_wac/1024^2))))
polygon(c(dd$full_date, rev(dd$full_date)),
    c(dd$dmin/1024^2, rev(dd$dmax/1024^2)), col="lightblue", border=NA)
with(dd, lines(full_date, size_wac/1024^2, col=1))
with(dd, points(full_date, size_wac/1024^2, pch=19,
    col=ifelse(pval < 0.05, 2, 1)))
box()
#abline(h=WAC_SIZE_MIN0/1024^2, col=2)
#lines(dd$full_date, dd$dmean/1024^2, col=4)

with(dd[dd$yday >= max(dd$yday)-7,], plot(full_date, size_wac/1024^2, type="n",
    main="", ylab="WAC file size (Mb)", xlab="1 week before retrieval",
    ylim=c(0, max(dd$size_wac/1024^2))))
polygon(c(dd$full_date, rev(dd$full_date)),
    c(dd$dmin/1024^2, rev(dd$dmax/1024^2)), col="lightblue", border=NA)
with(dd, lines(full_date, size_wac/1024^2, col=1))
with(dd, points(full_date, size_wac/1024^2, pch=19,
    col=ifelse(pval < 0.05, 2, 1)))
box()
#abline(h=WAC_SIZE_MIN0/1024^2, col=2)
#lines(dd$full_date, dd$dmean/1024^2, col=4)

par(op)
```

Identification of <3 minutes files is based on quantile-quantile plot ideas.
Here we sort the WAC file sizes from smallest to larges. Their rank gives 
quantile. This is then related to theoretical quantiles from a parametric
distribution, here a Normal distribution. It is also possible to
assign probabilities to the normalized observed file sizes 
(probability of coming from a standard Normal distribution).
The red dots indicate the outliers (also shown in the previous plots).
This process is really fast: 3.5 sec with scanning a directory with 1051 files.
This means 22 minutes for a full year worth of data.

```{r echo=FALSE,fig.height=5,fig.width=5}
plot(sid[,-3], type="l")
points(sid[,-3], pch=19, col=ifelse(sid[,3] < 0.05, 2, 1))
qqline(sid[,2], col=4)
```

To see if there is a file size threshold that can be applied as a
rule of thumb to quickly eliminate files by only sorting in file
explorer, I converted the flagged files to find out their true duration.
I also converted the smallest of the files that was not flagged to make 
sure I capture the low end of the file sizes for 3-minutes recordings.
I repeated this for 260 units and assessed accuracy of different tresholds
(a total of 4585 files, 1.2% smallest of all files, including the 260 random files).
I converted a maximum of 20 files per unit when the number of flagged small files
was high.

```{r echo=FALSE,fig.height=5,fig.width=5}
for (i in 1:length(RES)) {
    RESbyDIR[[i]][RESbyDIR[[i]]$file == RES[[i]]$file, "duration"] <- RES[[i]]$duration
}
zz <- do.call(rbind, lapply(RESbyDIR, function(z) {
    z[!is.na(z$duration),,drop=FALSE]
}))
zz$pval[is.na(zz$pval)] <- 1
zz$small <- zz$flag <- NULL
zz1 <- zz[zz$duration < 180,]
zz2 <- zz[zz$duration >= 180,]
```

The following figure shows the size distribution for files that are <3 minutes in
duration (in tomato color) and for longer (at least 3 minutes) files (grey).
Again, the maximum file size for unwanted files (Threshold1 = 17.7 Mb) is higher than the  
minimum file size for 3-minutes and above (Threshold2 = 11.2 Mb).

```{r echo=FALSE,fig.height=5,fig.width=7}
hist(zz2$size_wac/1024^2, col=rgb(190/255,190/255,190/255,0.6), breaks=seq(0,80,by=2),
    main="", xlab="WAC size (Mb)", xlim=c(0, max(zz2$size_wac/1024^2)), ylim=c(0,1500))
rug(zz2$size_wac/1024^2, col="grey", side=3)
hist(zz1$size_wac/1024^2, col=rgb(255/255,99/255,71/255,0.6), 
    add=TRUE, breaks=seq(0,80,by=2))
rug(zz1$size_wac/1024^2, col="tomato", side=1)
abline(v=min(zz2$size_wac/1024^2), col=4, lwd=2)
abline(v=max(zz1$size_wac/1024^2), col=4, lwd=2)
```

To assess accuracy for the two thresholds, calculated confusion matrices

```{r echo=FALSE}
True <- zz$duration < 180
Min3min <- zz$size_wac < min(zz2$size_wac)
MaxSmall <- zz$size_wac < max(zz1$size_wac)
n <- nrow(zz)
##  True pos           | False neg (Type II)
## --------------------+---------------------
##  False pos (Type I) | True neg
(cm2 <- round((table(Less_Than_3min=True, Less_Than_Threshold1=MaxSmall)/1)[c(2,1),c(2,1)],2))
## Accuracy = (True pos + True neg) / n
structure((Acc2 <- sum(diag(cm2)))/n, names="Accuracy_Threshold1")
(cm1 <- round((table(Less_Than_3min=True, Less_Than_Threshold2=Min3min)/1)[c(2,1),c(2,1)],2))
structure((Acc1 <- sum(diag(cm1)))/n, names="Accuracy_Threshold2")
```

Do accuracy for random sample only??? otherwise rates are misleading

Based on Threshold1, we would discard quite a few truly 3-minutes
files (the fals discovery rate needs to be assessed based on a random sample).
Fals negative rate (not surprisingly < 0.001) can however be assessed based on the small files,
because it contained almost all of the small files.

For Threshold2, FNR is 5.9% which is pretty good when none of the 3-minutes files
got into the mix.

Mixed approach: eliminate files <T2, inspect files between T1 and T2
to get rid of the rest of the unwanted (this is optional).

Show accuracy results

Present proposed process

Identify next steps
